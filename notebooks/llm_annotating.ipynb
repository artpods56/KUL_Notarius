{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d942b72",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "fb6d17c0",
   "metadata": {},
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from notarius import LayoutLMv3Interface, SQLite3Interface\n",
    "\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "layoutlmv3_interface = LayoutLMv3Interface(\n",
    "    model_path=os.path.join(ROOT_DIR, \"ml_models/layoutlmv3/checkpoint-400\"),\n",
    "    processor_path=\"microsoft/layoutlmv3-large\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "sqllite3_interface = SQLite3Interface(\n",
    "    database=os.path.join(ROOT_DIR, \"ecclesia.db\"),\n",
    ")\n",
    "\n",
    "SCHEMATISMS_DIR = os.path.join(ROOT_DIR, \"data/schematyzmy\")\n",
    "OCR_SCHEMATISMS_DIR = os.path.join(ROOT_DIR, \"data/ocr_schematyzmy\")\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"data/results\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "00cb5e23",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbb24d93",
   "metadata": {},
   "source": [
    "def visualize_tokens_with_labels(image_path, words, bboxes, labels, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize tokens with bounding boxes and colored labels on the image.\n",
    "    Ignores tokens with label \"O\".\n",
    "    Available labels: {'O', 'building_material', 'dedication', 'parish'}\n",
    "    \"\"\"\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "    # Define colors for each label\n",
    "    label2color = {\n",
    "        \"building_material\": \"red\",\n",
    "        \"dedication\": \"orange\",\n",
    "        \"parish\": \"blue\",\n",
    "        \"deanery\": \"green\",\n",
    "    }\n",
    "\n",
    "    # Open image\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"Arial\", 14)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        for word, bbox, label in zip(words, bboxes, labels):\n",
    "            if label == \"O\":\n",
    "                continue\n",
    "            color = label2color.get(label, \"black\")\n",
    "            draw.rectangle(bbox, outline=color, width=2)\n",
    "            # Draw label and word above the box\n",
    "            label_text = f\"{label}: {word}\"\n",
    "            # Use textbbox instead of textsize\n",
    "            left, top, right, bottom = draw.textbbox((0, 0), label_text, font=font)\n",
    "            text_width = right - left\n",
    "            text_height = bottom - top\n",
    "            text_x, text_y = bbox[0], max(0, bbox[1] - text_height)\n",
    "            draw.rectangle(\n",
    "                [text_x, text_y, text_x + text_width, text_y + text_height],\n",
    "                fill=(255, 255, 255, 180),\n",
    "            )\n",
    "            draw.text((text_x, text_y), label_text, fill=color, font=font)\n",
    "\n",
    "        if output_path:\n",
    "            img.save(output_path)\n",
    "        return img"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "377d2a53",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "id": "7af2aaea",
   "metadata": {},
   "source": [
    "system_prompt = \"\"\"You are a specialized extraction assistant that identifies and labels specific information from 19th‑century Latin ecclesiastical schematisms (diocesan notices).\n",
    "\n",
    "Your task is to extract labeled text spans from Latin‑Polish diocesan notices and return them as a JSON list with positional information. You must align Latin terms in the source text with their Polish equivalents in the ground‑truth table.\n",
    "\n",
    "---\n",
    "\n",
    "## Labels and Their Meanings\n",
    "\n",
    "| Label                  | What to Extract                    | Latin Format Examples                                                                               | Polish Translation              |\n",
    "| ---------------------- | ---------------------------------- | --------------------------------------------------------------------------------------------------- | ------------------------------- |\n",
    "| **parish**             | Parish description                        | Usually capitalized; follows the ordinal number **or** appears as the first word in the notice line | Direct description (may vary slightly) |\n",
    "| **page_number**       | Page number in the schematism      | Usually numeric, often first in notice                                                              | Same number                     |\n",
    "| **dedication**         | Church's dedication / patron saint | `S.`, `SS.`, `B.M.V.`, `Nativ.` etc.                                                                | Full saint description in Polish       |\n",
    "| **building_material** | Church construction material       | `mur.`, `mr`, `murata`, `ex muro` (masonry) — `lig.`, `l.`, `dr`, `lignea` (wood)                   | `murowany` / `drewniany`        |\n",
    "| **deanery**            | Deanery description                       | Appears after `dekanat` or `decanatus`                                                              | Name in Polish                  |\n",
    "\n",
    "---\n",
    "\n",
    "## **Parsing Rules** (supersede earlier versions)\n",
    "\n",
    "1. **Notice boundaries**\n",
    "   *Ignore headings.* Textual headers such as `ECCLESIA Cathedr. …`, `PAROCHUS …`, etc. belong to the preceding context and **must be ignored** for extraction purposes. The *notice proper* begins at the first line that either:\n",
    "\n",
    "   * starts with an ordinal number (`\"1.\", \"2.\", \"—\"`, etc.), **or**\n",
    "   * starts with the parish description in its Polish nominative form followed by a comma.\n",
    "\n",
    "2. **Parish description (form preference)**\n",
    "\n",
    "   * Extract the **nominative form that appears verbatim** in the tokens, favouring the Polish spelling with diacritics (e.g., `Tarnów`).\n",
    "   * Disregard Latinised genitive/locative endings such as `-ae`, `-i`, `-ensis`, etc. (`Tarnoviae`, `Cracoviae`) **unless** no Polish nominative form occurs anywhere in the notice.\n",
    "\n",
    "3. **Ground‑truth alignment**\n",
    "\n",
    "   * If the Polish nominative form is present, extract that form so that it matches the Polish ground‑truth table.\n",
    "   * If only a Latin form is available, extract that Latin form **and** update the ground‑truth table accordingly (outside this assistant).\n",
    "\n",
    "4. **Other extraction constraints** \n",
    "\n",
    "   1. Extract **only** within the boundaries of a single notice.\n",
    "   2. Never split or merge tokens that are pre‑segmented in the input.\n",
    "   3. Match Latin terminology with corresponding Polish concepts in the ground truth.\n",
    "   4. Include positional information (word indices) for each extraction.\n",
    "   5. Omit any text that doesn't match the specific labels.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Few‑shot Examples\n",
    "# EXAMPLE 1\n",
    "### Ground Truth (Polish)\n",
    "[{'deanery': 'Dąbrowa', 'parish': 'Bolesław', 'dedication': 'Wojciech Biskup Męczennik', 'material_type': 'mr'}]\n",
    "### Input text\n",
    "[TEXT_START]\n",
    "In Circulo quondam Tarnoviensi.\n",
    "\n",
    "1. Decanatus Dabrovaensis.\n",
    "\n",
    "1. Bolesław, P. E. p. mur. — a. 1632 per Stanisl,\n",
    "Ligęza e ligno aedif. 22. Oct. 1634 per Thom. Oborski\n",
    "Ep. Laodicens. cons. dein desolata et combusta, — a. 1731\n",
    "mur. et per Joann. Skarbek Arcbi-Epp. Leopoliens. cons.\n",
    "Jam vero a. 1326 in opere per Aug. Theiner edito de hac\n",
    "ecclesia et curato mentio fit. T. E. S. Adalbertus E. M,\n",
    "Matr. ex a° 1648. Patr. T. D. Marianus Eques de Sroczyński.\n",
    "[TEXT_END]\n",
    "### OUTPUT\n",
    "[\n",
    "  {\"label\":\"deanery\", \"text\":\"Decanatus Dabrovaensis.\",  \"text_match_patch: \"In Circulo quondam Tarnoviensi.\\n1. <DEANERY>Decanatus Dabrovaensis.</DEANERY>\\n1. Bolesław\" ,\n",
    "  {\"label\":\"building_material\", \"text\":\"mur.\",  \"text_match_patch: \"combusta, — a. 1731\\n<BUILDING_MATERIAL>mur.</BUILDING_MATERIAL> et per\" ,\n",
    "  {\"label\":\"parish\", \"text\":\"Bolesław\", \"text_match_patch: \"Decanatus Dabrovaensis.\\n1. <PARISH>Bolesław</PARISH>, P. E.\" ,\n",
    "  {\"label\":\"dedication\", \"text\":\"T. E. S. Adalbertus E. M,\",  \"text_match_patch: \"mentio fit. <DEDICATION>T. E. S. Adalbertus E. M,</DEDICATION>\\nMatr.\" ,\n",
    "]\n",
    "\n",
    "# EXAMPLE 2\n",
    "### Ground Truth (Polish)\n",
    "\n",
    "Ground truth:  \n",
    "{'deanery': 'Dąbrowa', 'parish': 'Gręboszów', 'dedication': 'Najświętsza Maryja Panna Wniebowzięta', 'material_type': 'mr'}, \n",
    "{'deanery': 'Dąbrowa', 'parish': 'Dąbrowa', 'dedication': 'Wszyscy Święci', 'material_type': 'dr'}]\n",
    "### Input text\n",
    "[TEXT_START]\n",
    "140. Mẹdrzychów Y, 1240. Kupienin ®/, m. 460. — Univ.\n",
    "Cath. 6206. Jud, 435.\n",
    "\n",
    "Capitaneatus districtualis et off. postale Dąbrowa.\n",
    "\n",
    "2. Dabrowa, 0. E. p. lign. A. E. ign. olim praep.\n",
    "cum proprio Promot. SS. Rosarii. Eccl. antiqua per Nicolaum\n",
    "Spytek Ligęza Casteļlanum dotata, 1614 per Valerianum\n",
    "Lubieniecki Epp. Bacoviensem cons. ob vetustatem desolata,\n",
    "nova amplior 1774. per Cajetanum Potocki Canon. Cracov.\n",
    "de ligno extructa, per Gregorium Thomam Ziegler Epp. Ty-\n",
    "niec. 1824. cons. T. E. 00. SS, Matr. ant. ex a. 1611.\n",
    "Patr. T. D. Eugenius de Jordan Stojowski.\n",
    "Capitan. distr. et off. post. Dabrowa.\n",
    "\n",
    "3. Greboszów, P. E. p. mur. A. E. ignot. ast\n",
    "ante annum 1326, existens, juxta Theiner tom. I. p. 252.\n",
    "Praesens eccl. a Francisco de Dembiany Dembiński Palat,\n",
    "Cracov. 1650 aedificata, per Nicol. Oborski Epp. Laodicen.\n",
    "1675 in honorem Assumptionis B. M. V. cons. Matr. ant.\n",
    "Nator. -ex a. 165t. Patr. T. D. Sophia Comitissa Załuska,\n",
    "[TEXT_END]\n",
    "### OUTPUT\n",
    "[\n",
    "   {\"label\":\"parish\", \"text\":\"Dąbrowa\", \"text_match_patch: \"postale Dąbrowa. 2. <PARISH>Dabrowa</PARISH>, 0. E.\" ,\n",
    "   {\"label\":\"building_material\", \"text\":\"lign.\",  \"text_match_patch: \"E. p. <BUILDING_MATERIAL>lign.<BUILDING_MATERIAL> A. E. ign.\" ,\n",
    "   {\"label\":\"dedication\", \"text\":\"T. E. 00. SS.\",  \"text_match_patch: \"1824. cons. <DEDICATION>T. E. 00. SS.</DEDICATION> Matr. ant.\" ,\n",
    "   {\"label\":\"parish\", \"text\":\"Greboszów\", \"text_match_patch: \"post. Dabrowa. 3.<PARISH>Greboszów</PARISH>, P. E.\" ,\n",
    "   {\"label\":\"building_material\", \"text\":\"mur.\",  \"text_match_patch: \"E. p. <BUILDING_MATERIAL>mur.</BUILDING_MATERIAL> A. E. ignot.\" ,\n",
    "   {\"label\": \"dedication\", \"text\":\"Assumptionis B. M. V.\",  \"text_match_patch: \"in honorem <DEDICATION>Assumptionis B. M. V.</DEDICATION> cons.\" ,}\n",
    "]\n",
    "\n",
    "## END OF EXAMPLES\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b04f2e23",
   "metadata": {},
   "source": [
    "user_prompt = \"\"\"\n",
    "## User Prompt\n",
    "\n",
    "Extract and label information from this Latin ecclesiastical schematism that corresponds to the provided ground truth table in Polish.\n",
    "\n",
    "### Ground Truth (Polish)\n",
    "{ground_truth}\n",
    "\n",
    "\n",
    "### Instructions\n",
    "1. Identify Latin terms in the tokens that correspond to the Polish ground truth information.\n",
    "2. For \"material_type\", look for terms like \"mur.\", \"mr\", \"murata\" (for masonry) or \"lig.\", \"dr\", \"lignea\" (for wood).\n",
    "3. For \"dedication\", find Latin abbreviations like \"S.\", \"SS.\", \"B.M.V.\", \"Nativ.\" that represent the Polish saint/dedication.\n",
    "4. The \"parish\" description ussually is the same as the one in the ground truth table. It is very important to match the description with the token sequence.\n",
    "5. Return only a JSON array with labeled items - exclude anything that doesn't match the required labels.\n",
    "6. Dont output the ''' json beggin''' and '''json end''' tags.\n",
    "\n",
    "### Input text\n",
    "[TEXT_START]\n",
    "{input_text}\n",
    "[TEXT_END]\n",
    "\n",
    "I need a JSON output containing only the labeled information with text exactly as found in the tokens, remember to include the `text_match_patch` with valid text segment.. Response should include only the JSON array, nothing else. The ouput should be a valid JSON array.\n",
    "Return **only** a JSON array where each element is an object with:\n",
    "\n",
    "* `label`: one of the defined labels\n",
    "* `text`: the exact span as it appears in the input tokens (subject to Rule 2)\n",
    "* `text_match_patch`: the text span with `<LABEL>`…`</LABEL>` tags, sufficient to locate it in context, be sure to include the text before and after the match, **only one pair of tags per match**, should contain around 5 tokens before and after the match, but not more than 10 tokens in total.\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17245551",
   "metadata": {},
   "source": [
    "# Pytesseract Interface"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f8e1c5d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cbaf1ce0",
   "metadata": {},
   "source": [
    "# LLM Interface"
   ]
  },
  {
   "cell_type": "code",
   "id": "4dda85d0",
   "metadata": {},
   "source": [
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "import logging\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client,\n",
    "        model_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        model_parameters: dict = None,\n",
    "        stream: bool = False,\n",
    "        no_think: bool = False,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"text\": self.system_prompt},\n",
    "        ]\n",
    "        self.model_parameters = model_parameters\n",
    "        self.no_think = no_think\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\"[%(levelname)s] %(message)s\")\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def stream_response(self):\n",
    "        self.logger.info(\"\\n\" + \"-\" * 20 + \" Start of streamed output \" + \"-\" * 20)\n",
    "        if isinstance(self.client, OpenAI):\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name, messages=self.messages, **self.model_parameters\n",
    "            )\n",
    "        elif isinstance(self.client, Mistral):\n",
    "            response = self.client.chat.stream(\n",
    "                model=self.model_name, messages=self.messages, **self.model_parameters\n",
    "            )\n",
    "\n",
    "        complete_response = \"\"\n",
    "        for streamed_response in response:\n",
    "            if isinstance(self.client, OpenAI):\n",
    "                streamed_chunk = streamed_response.choices[0].delta.content\n",
    "            elif isinstance(self.client, Mistral):\n",
    "                streamed_chunk = streamed_response.data.choices[0].delta.content\n",
    "\n",
    "            if streamed_chunk:\n",
    "                if self.verbose:\n",
    "                    print(streamed_chunk, end=\"\", flush=True)\n",
    "                complete_response += streamed_chunk\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\" \", end=\"\", flush=True)\n",
    "        self.logger.info(\"\\n\" + \"-\" * 20 + \" End of streamed output \" + \"-\" * 20)\n",
    "\n",
    "        return complete_response\n",
    "\n",
    "    def completions_response(self):\n",
    "        self.logger.info(\"\\n\" + \"-\" * 20 + \" Start of non-streamed output \" + \"-\" * 20)\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                if isinstance(self.client, Mistral):\n",
    "                    response = self.client.chat.complete(\n",
    "                        model=self.model_name,\n",
    "                        messages=self.messages,\n",
    "                        **self.model_parameters,\n",
    "                    )\n",
    "                    response_content = response.choices[0].message.content\n",
    "                    break\n",
    "                elif isinstance(self.client, OpenAI):\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model_name,\n",
    "                        messages=self.messages,\n",
    "                        **self.model_parameters,\n",
    "                    )\n",
    "                    response_content = response.choices[0].message.content\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error generating output: {e}\")\n",
    "                if i == 2:\n",
    "                    raise\n",
    "        self.logger.info(response_content)\n",
    "        self.logger.info(\"\\n\" + \"-\" * 20 + \" End of non-streamed output \" + \"-\" * 20)\n",
    "        return response_content\n",
    "\n",
    "    def process_response(self, response):\n",
    "        import re\n",
    "        import json\n",
    "\n",
    "        # get rid of the <think> and </think> tags\n",
    "        json_response = re.sub(\n",
    "            r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL | re.MULTILINE\n",
    "        ).strip()\n",
    "        return json.loads(json_response)\n",
    "\n",
    "    def generate(self):\n",
    "        self.logger.info(\n",
    "            f\"Model description: {self.model_name} - {self.client.__class__.__name__}\"\n",
    "        )\n",
    "        if self.model_parameters[\"stream\"]:\n",
    "            response = self.stream_response()\n",
    "        else:\n",
    "            response = self.completions_response()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_labeles(self, page_ocr: str, ground_truth: str):\n",
    "        self.messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"text\": self.user_prompt.format(\n",
    "                    input_text=page_ocr, ground_truth=ground_truth\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        if self.no_think:\n",
    "            self.messages[-1][\"text\"] += \"/no_think\"\n",
    "        response = self.generate()\n",
    "\n",
    "        json_response = self.process_response(response)\n",
    "        self.logger.info(f\"Json output: {json_response}\")\n",
    "        return json_response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7d9798c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8889f42",
   "metadata": {},
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing trailing punctuation\n",
    "    3. Removing diacritics\n",
    "    4. Standardizing whitespace\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove diacritics (normalize unicode characters)\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "\n",
    "    # Remove trailing punctuation and standardize whitespace\n",
    "    text = re.sub(r\"[.,;:]+$\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_fuzzy_span_indices(span, tokens, context=None, threshold=60):\n",
    "    \"\"\"Find token indices that best match the span, using context if provided.\"\"\"\n",
    "    # Normalize the span and its tokens\n",
    "    span_normalized = normalize_text(span)\n",
    "    span_tokens = span_normalized.split()\n",
    "    n = len(span_tokens)\n",
    "\n",
    "    # Normalize all input tokens\n",
    "    tokens_normalized = [normalize_text(t) for t in tokens]\n",
    "\n",
    "    best_score = -1\n",
    "    best_indices = []\n",
    "\n",
    "    # Slide through tokens looking for matches\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        window = tokens[i : i + n]\n",
    "        window_normalized = \" \".join(tokens_normalized[i : i + n])\n",
    "\n",
    "        # Calculate base similarity score using normalized text\n",
    "        score = fuzz.ratio(span_normalized, window_normalized)\n",
    "\n",
    "        # Check context if provided\n",
    "        if context and score >= threshold:\n",
    "            before, after = context\n",
    "            before_normalized = normalize_text(before)\n",
    "            after_normalized = normalize_text(after)\n",
    "\n",
    "            if before:\n",
    "                before_window = \" \".join(\n",
    "                    tokens_normalized[max(0, i - len(before.split())) : i]\n",
    "                )\n",
    "                if (\n",
    "                    fuzz.ratio(before_normalized, before_window) < 70\n",
    "                ):  # Stricter threshold\n",
    "                    score -= 20\n",
    "\n",
    "            if after:\n",
    "                after_window = \" \".join(\n",
    "                    tokens_normalized[i + n : i + n + len(after.split())]\n",
    "                )\n",
    "                if fuzz.ratio(after_normalized, after_window) < 70:\n",
    "                    score -= 20\n",
    "\n",
    "        if score > best_score and score >= threshold:\n",
    "            best_score = score\n",
    "            best_indices = list(range(i, i + n))\n",
    "\n",
    "    return best_indices\n",
    "\n",
    "\n",
    "def extract_context_from_patch(patch, span):\n",
    "    \"\"\"Extract normalized context before and after the labeled span.\"\"\"\n",
    "    before = after = \"\"\n",
    "    match = re.search(r\"(.*)<[^>]*>%s</[^>]*>(.*)\" % re.escape(span), patch, flags=re.S)\n",
    "    if match:\n",
    "        # Get context words (up to 3 words before and after)\n",
    "        before_words = match.group(1).strip().split()\n",
    "        after_words = match.group(2).strip().split()\n",
    "\n",
    "        before = \" \".join(before_words)\n",
    "        after = \" \".join(after_words)\n",
    "\n",
    "    return before, after"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63b58d26",
   "metadata": {},
   "source": [
    "lm_studio_client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",  # LM Studio doesn’t enforce a real key\n",
    ")\n",
    "\n",
    "\n",
    "labeler_schema = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"description\": \"annotations\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"label\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\n",
    "                            \"parish\",\n",
    "                            \"building_material\",\n",
    "                            \"dedication\",\n",
    "                            \"deanery\",\n",
    "                            \"page_number\",\n",
    "                        ],\n",
    "                    },\n",
    "                    \"text\": {\"type\": \"string\"},\n",
    "                    \"text_match_patch\": {\"type\": \"string\"},\n",
    "                },\n",
    "                \"required\": [\"label\", \"text\", \"text_match_patch\"],\n",
    "            },\n",
    "            \"minItems\": 1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "lm_studio_parameters = {\n",
    "    # \"temperature\": 0.0,\n",
    "    # \"top_p\": 0.9,\n",
    "    # \"stop\": [\"\\n\\n\"],\n",
    "    \"stream\": True,\n",
    "    \"response_format\": labeler_schema,\n",
    "}\n",
    "\n",
    "# llm = LLM(client=lm_studio_client,\n",
    "#           model_name=\"qwen3-30b-a3b-mlx\",\n",
    "#           system_prompt=system_prompt,\n",
    "#           user_prompt=user_prompt,\n",
    "#           model_parameters=lm_studio_parameters,\n",
    "#           verbose=True,\n",
    "#           no_think=True,\n",
    "#           )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "026511e6",
   "metadata": {},
   "source": [
    "# Client setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "099736c1",
   "metadata": {},
   "source": [
    "mistral_client = Mistral(api_key=\"\")\n",
    "\n",
    "mistral_parameters = {\n",
    "    \"temperature\": 0.5,\n",
    "    # \"frequency_penalty\": 1.2,  # Add this\n",
    "    # \"presence_penalty\": 0.6,\n",
    "    # \"top_p\": 0.9,\n",
    "    # \"stop\": [\"\\n\\n\"],\n",
    "    \"stream\": True,\n",
    "    \"response_format\": {\n",
    "        \"type\": \"json_object\",\n",
    "    },\n",
    "}\n",
    "\n",
    "llm = LLM(\n",
    "    client=mistral_client,\n",
    "    model_name=\"mistral-large-latest\",\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    model_parameters=mistral_parameters,\n",
    "    verbose=True,\n",
    "    no_think=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7fc661a0",
   "metadata": {},
   "source": [
    "# Annotation specification"
   ]
  },
  {
   "cell_type": "code",
   "id": "e917f981",
   "metadata": {},
   "source": [
    "schematisms_to_evaluate = [\"wloclawek_1872\"]\n",
    "LLM_ANNOTATIONS_DIR = os.path.join(ROOT_DIR, \"data/llm_annotations\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ac7bb2b",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce763e0e",
   "metadata": {},
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "442059c5",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "89982859",
   "metadata": {},
   "source": [
    "for schematism in schematisms_to_evaluate:\n",
    "    geometries = sqllite3_interface.query(\n",
    "        [\"id\", \"dekanat\", \"miejsce\", \"wezwanie\", \"material_typ\", \"the_geom\", \"skany\"],\n",
    "        {\"skany\": f\"'{schematism}'\"},\n",
    "        table_name=\"dane_hasla\",\n",
    "    )\n",
    "    shapefile_path = os.path.join(SCHEMATISMS_DIR, schematism, \"matryca/matryca.shp\")\n",
    "\n",
    "    shp_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    import geopandas as gpd\n",
    "    from shapely import wkt\n",
    "\n",
    "    geom_list = []\n",
    "    for geom in geometries:\n",
    "        _, deanery, parish, dedication, material_type, geom, schematism = geom\n",
    "\n",
    "        try:\n",
    "            geom = wkt.loads(geom)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading WKT: {geom}\")\n",
    "            logger.error(str(e))\n",
    "            continue\n",
    "        geom_list.append(\n",
    "            {\n",
    "                \"deanery\": deanery,\n",
    "                \"parish\": parish,\n",
    "                \"dedication\": dedication,\n",
    "                \"material_type\": material_type,\n",
    "                \"geom\": geom,\n",
    "                \"schematism\": schematism,\n",
    "            }\n",
    "        )\n",
    "        logger.debug(f\"Geometry loaded: {geom}\")\n",
    "\n",
    "    sql_gdf = gpd.GeoDataFrame(geom_list, geometry=\"geom\", crs=shp_gdf.crs)\n",
    "\n",
    "    joined_gdf = gpd.sjoin(shp_gdf, sql_gdf, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "    for file_name in sorted(os.listdir(os.path.join(SCHEMATISMS_DIR, schematism))):\n",
    "        annotation_file = os.path.join(\n",
    "            LLM_ANNOTATIONS_DIR, schematism, file_name.replace(\".jpg\", \".json\")\n",
    "        )\n",
    "        if os.path.exists(annotation_file):\n",
    "            logger.info(f\"Skipping existing annotation file: {annotation_file}\")\n",
    "            continue\n",
    "\n",
    "        pytesseract_ocr = PytesseractOCRInterface(\n",
    "            schematisms_source_dir=SCHEMATISMS_DIR,\n",
    "            schematisms_ocr_target_dir=OCR_SCHEMATISMS_DIR,\n",
    "            schematism=schematism,\n",
    "            langs=[\"pol\", \"lat\"],\n",
    "            force_ocr=False,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        if not file_name.endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(SCHEMATISMS_DIR, schematism, file_name)\n",
    "        page_ground_truth = joined_gdf[joined_gdf[\"location\"] == file_name]\n",
    "\n",
    "        if not page_ground_truth.empty:\n",
    "            logger.info(f\"\\n{'=' * 80}\\nProcessing {file_name}...\\n{'=' * 80}\")\n",
    "\n",
    "            ocr_data = pytesseract_ocr.load_ocr_data(file_name)\n",
    "            tokens = ocr_data[\"words\"]\n",
    "            labels = [\"O\"] * len(ocr_data[\"words\"])\n",
    "\n",
    "            # Clean up ground truth data\n",
    "            clean_ground_truth = page_ground_truth.drop(\n",
    "                columns=[\"geometry\", \"location\", \"index_right\", \"schematism\"]\n",
    "            ).to_markdown(index=False)\n",
    "            logger.info(\n",
    "                f\"\\nGround Truth:\\n{'-' * 40}\\n{clean_ground_truth}\\n{'-' * 40}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"\\nOCR Text:\\n{'-' * 40}\\n{ocr_data['complete_text']}\\n{'-' * 40}\"\n",
    "            )\n",
    "\n",
    "            labels_json = llm.get_labeles(\n",
    "                page_ocr=ocr_data[\"complete_text\"], ground_truth=clean_ground_truth\n",
    "            )\n",
    "            llm.messages.pop(-1)  # Remove the last user message\n",
    "\n",
    "            for annotation in labels_json:\n",
    "                if isinstance(annotation, str):\n",
    "                    try:\n",
    "                        annotation = json.loads(annotation.strip())\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        logger.error(f\"Error decoding JSON output: {e}\")\n",
    "                        continue\n",
    "\n",
    "                label = annotation[\"label\"]\n",
    "                patch = annotation[\"text_match_patch\"]\n",
    "\n",
    "                span = re.search(r\"<[^>]*>(.*?)</\", patch, flags=re.S)\n",
    "                if not span:\n",
    "                    logger.error(f\"Error: No span found in patch: {patch}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    span = span.group(1)\n",
    "\n",
    "                # Extract context and find matching tokens\n",
    "                before, after = extract_context_from_patch(patch, span)\n",
    "                matching_indices = find_fuzzy_span_indices(\n",
    "                    span, tokens, context=(before, after)\n",
    "                )\n",
    "\n",
    "                # Apply labels to matching tokens\n",
    "                for idx in matching_indices:\n",
    "                    labels[idx] = label\n",
    "\n",
    "            if not os.path.exists(os.path.join(LLM_ANNOTATIONS_DIR, schematism)):\n",
    "                os.makedirs(os.path.join(LLM_ANNOTATIONS_DIR, schematism))\n",
    "\n",
    "            annotated_image = visualize_tokens_with_labels(\n",
    "                image_path,\n",
    "                ocr_data[\"words\"],\n",
    "                ocr_data[\"bboxes\"],\n",
    "                labels,\n",
    "                output_path=None,\n",
    "            )\n",
    "            display(annotated_image)\n",
    "\n",
    "            with open(annotation_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                content = {\n",
    "                    \"words\": tokens,\n",
    "                    \"bboxes\": ocr_data[\"bboxes\"],\n",
    "                    \"labels\": labels,\n",
    "                }\n",
    "                json.dump(content, f, ensure_ascii=False, indent=4)\n",
    "                logger.info(f\"\\nAnnotation file saved: {annotation_file}\")\n",
    "        else:\n",
    "            logger.debug(f\"No ground truth data found for {file_name}\")\n",
    "            continue"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
